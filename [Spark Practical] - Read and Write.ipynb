{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef200388-7ab6-41ee-a141-05d243a96826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=6537966655703261#setting/sparkui/1219-070709-rai6rrsi/driver-6750455792685708733\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=6537966655703261#setting/sparkui/1219-070709-rai6rrsi/driver-6750455792685708733\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0fc6479-f748-4d17-bfa9-b382deac6345",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|              _c0|                _c1|  _c2|\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n|    United States|       Saint Martin|    2|\n|    United States|             Guinea|    2|\n|    United States|            Croatia|    1|\n|    United States|            Romania|    3|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df = spark.read.format('csv')\\\n",
    "        .option(\"header\",'false')\\\n",
    "        .option('inferschema','false')\\\n",
    "        .option('mode','failfast')\\\n",
    "        .load('/FileStore/tables/2011_summary.csv')\n",
    "\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89fa368d-aae7-46fc-aa3c-4aa9cc1fc811",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|       Saint Martin|    2|\n|    United States|             Guinea|    2|\n|    United States|            Croatia|    1|\n|    United States|            Romania|    3|\n|    United States|            Ireland|  268|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header = spark.read.format('csv')\\\n",
    "        .option(\"header\",'true')\\\n",
    "        .option('inferschema','false')\\\n",
    "        .option('mode','failfast')\\\n",
    "        .load('/FileStore/tables/2011_summary.csv')\n",
    "\n",
    "flight_df_header.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1b9edf-2c91-4acd-b6be-38701994a521",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7abf8826-674d-48ee-b181-ad0180c58564",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|       Saint Martin|    2|\n|    United States|             Guinea|    2|\n|    United States|            Croatia|    1|\n|    United States|            Romania|    3|\n|    United States|            Ireland|  268|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header_schema = spark.read.format('csv')\\\n",
    "        .option(\"header\",'true')\\\n",
    "        .option('inferschema','true')\\\n",
    "        .option('mode','failfast')\\\n",
    "        .load('/FileStore/tables/2011_summary.csv')\n",
    "\n",
    "flight_df_header_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10216621-01ee-4fad-b999-554bf7f53e93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "544cb5b8-93bb-4813-b31f-6d38fcbe6b98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4299228551272091>:4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m flight_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n",
       "\u001B[1;32m      2\u001B[0m                 \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n",
       "\u001B[1;32m      3\u001B[0m                 \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minferschema\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n",
       "\u001B[0;32m----> 4\u001B[0m                 \u001B[38;5;241m.\u001B[39mschema(create_schema)\\\n",
       "\u001B[1;32m      5\u001B[0m                 \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n",
       "\u001B[1;32m      6\u001B[0m                 \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/tables/2011_summary.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m flight_df\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'create_schema' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4299228551272091>:4\u001B[0m\n\u001B[1;32m      1\u001B[0m flight_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n\u001B[1;32m      2\u001B[0m                 \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n\u001B[1;32m      3\u001B[0m                 \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minferschema\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n\u001B[0;32m----> 4\u001B[0m                 \u001B[38;5;241m.\u001B[39mschema(create_schema)\\\n\u001B[1;32m      5\u001B[0m                 \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n\u001B[1;32m      6\u001B[0m                 \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/tables/2011_summary.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      8\u001B[0m flight_df\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n\n\u001B[0;31mNameError\u001B[0m: name 'create_schema' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'create_schema' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flight_df = spark.read.format('csv')\\\n",
    "                .option('header','false')\\\n",
    "                .option('inferschema','false')\\\n",
    "                .schema(create_schema)\\\n",
    "                .option('mode','FAILFAST')\\\n",
    "                .load('/FileStore/tables/2011_summary.csv')\n",
    "\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe00fead-90c7-4449-aa99-2e622a136d89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de79c85f-cc1e-4490-b31e-153d1b7ec7d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_schema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(),True),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('count',IntegerType(), True)\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ad59d9-9a5a-4071-be66-29c860a5abff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n|    United States|       Saint Martin|    2|\n|    United States|             Guinea|    2|\n|    United States|            Croatia|    1|\n|    United States|            Romania|    3|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df = spark.read.format('csv')\\\n",
    "                .option('header','false')\\\n",
    "                .option('inferschema','false')\\\n",
    "                .schema(create_schema)\\\n",
    "                .option('mode','PERMISSIVE')\\\n",
    "                .load('/FileStore/tables/2011_summary.csv')\n",
    "\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339ad548-7d0e-446d-8fae-ad461b5194f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/2011_summary.csv</td><td>2011_summary.csv</td><td>7069</td><td>1702969429000</td></tr><tr><td>dbfs:/FileStore/tables/emp_data-1.csv</td><td>emp_data-1.csv</td><td>230</td><td>1703056833000</td></tr><tr><td>dbfs:/FileStore/tables/emp_data.csv</td><td>emp_data.csv</td><td>230</td><td>1703056812000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/2011_summary.csv",
         "2011_summary.csv",
         7069,
         1702969429000
        ],
        [
         "dbfs:/FileStore/tables/emp_data-1.csv",
         "emp_data-1.csv",
         230,
         1703056833000
        ],
        [
         "dbfs:/FileStore/tables/emp_data.csv",
         "emp_data.csv",
         230,
         1703056812000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ae52e8-6546-4cab-bb60-ffebdf05987f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|            Romania|    3|\n|    United States|            Ireland|  268|\n|            Egypt|      United States|   13|\n|    United States|              India|   76|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df = spark.read.format('csv')\\\n",
    "                .option('header','false')\\\n",
    "                .option('skipRows',3)\\\n",
    "                .option('inferschema','false')\\\n",
    "                .schema(create_schema)\\\n",
    "                .option('mode','PERMISSIVE')\\\n",
    "                .load('/FileStore/tables/2011_summary.csv')\n",
    "\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "038706f5-65ad-452c-ae01-269d876e83dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "my_schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('salary', IntegerType(), True),\n",
    "    StructField('address', StringType(), True),\n",
    "    StructField('nominee', StringType(), True),\n",
    "    StructField('_corrupt_record', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2256962f-a72b-407f-836a-a02daffb0541",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###_corrupt_record\n",
    "##.option('columnNameOfCorruptRecord','ABC')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd709f0-67b5-474e-bbb2-3e9503a0dfa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+------------+--------+-------------------------------------------+\n|id |name    |age|salary|address     |nominee |_corrupt_record                            |\n+---+--------+---+------+------------+--------+-------------------------------------------+\n|1  |Manish  |26 |75000 |bihar       |nominee1|null                                       |\n|2  |Nikita  |23 |100000|uttarpradesh|nominee2|null                                       |\n|3  |Pritam  |22 |150000|Bangalore   |India   |3,Pritam,22,150000,Bangalore,India,nominee3|\n|4  |Prantosh|17 |200000|Kolkata     |India   |4,Prantosh,17,200000,Kolkata,India,nominee4|\n|5  |Vikash  |31 |300000|null        |nominee5|null                                       |\n+---+--------+---+------+------------+--------+-------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_corrupt_data = spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','false')\\\n",
    "                        .schema(my_schema)\\\n",
    "                        .option('mode','permissive')\\\n",
    "                        .load('/FileStore/tables/emp_data.csv')\n",
    "\n",
    "df_corrupt_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b6fd10-ce54-462b-a9a1-b0711c559504",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## store bad records -- stores in JSON always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32781fb-94e1-4359-98ce-3ce26730976a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+------------+--------+---------------+\n|id |name  |age|salary|address     |nominee |_corrupt_record|\n+---+------+---+------+------------+--------+---------------+\n|1  |Manish|26 |75000 |bihar       |nominee1|null           |\n|2  |Nikita|23 |100000|uttarpradesh|nominee2|null           |\n|5  |Vikash|31 |300000|null        |nominee5|null           |\n+---+------+---+------+------------+--------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_corrupt_data = spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','false')\\\n",
    "                        .schema(my_schema)\\\n",
    "                        .option('badRecordsPath','/FileStore/tables/bad_records')\\\n",
    "                        .load('/FileStore/tables/emp_data.csv')\n",
    "df_corrupt_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f6ea47-1f3b-4d51-a34d-e0e012bd4e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/2011_summary.csv</td><td>2011_summary.csv</td><td>7069</td><td>1702969429000</td></tr><tr><td>dbfs:/FileStore/tables/bad_records/</td><td>bad_records/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/emp_data-1.csv</td><td>emp_data-1.csv</td><td>230</td><td>1703056833000</td></tr><tr><td>dbfs:/FileStore/tables/emp_data.csv</td><td>emp_data.csv</td><td>230</td><td>1703056812000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/2011_summary.csv",
         "2011_summary.csv",
         7069,
         1702969429000
        ],
        [
         "dbfs:/FileStore/tables/bad_records/",
         "bad_records/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/emp_data-1.csv",
         "emp_data-1.csv",
         230,
         1703056833000
        ],
        [
         "dbfs:/FileStore/tables/emp_data.csv",
         "emp_data.csv",
         230,
         1703056812000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d68409-e7a1-4965-812b-87720a9d0bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/bad_records/20231220T200121/bad_records/part-00000-f654e96d-15c9-4e17-9d73-1f8a6aef4d13</td><td>part-00000-f654e96d-15c9-4e17-9d73-1f8a6aef4d13</td><td>484</td><td>1703102483000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/bad_records/20231220T200121/bad_records/part-00000-f654e96d-15c9-4e17-9d73-1f8a6aef4d13",
         "part-00000-f654e96d-15c9-4e17-9d73-1f8a6aef4d13",
         484,
         1703102483000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/tables/bad_records/20231220T200121/bad_records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65bc49a8-0285-4885-9e38-839cfa8d0803",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n|path                               |reason                                                                                                                          |record                                     |\n+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n|dbfs:/FileStore/tables/emp_data.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 3,Pritam,22,150000,Bangalore,India,nominee3|3,Pritam,22,150000,Bangalore,India,nominee3|\n|dbfs:/FileStore/tables/emp_data.csv|org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 4,Prantosh,17,200000,Kolkata,India,nominee4|4,Prantosh,17,200000,Kolkata,India,nominee4|\n+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "bad_data_df = spark.read.format('json').load('/FileStore/tables/bad_records/20231220T200121/bad_records/')\n",
    "\n",
    "bad_data_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fecbe0-90d8-4abf-b96b-0c008fbfa9a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:557)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:673)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:566)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:557)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:527)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1572003811895331:1)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1572003811895331:43)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1572003811895331:45)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw$$iw.&lt;init&gt;(command-1572003811895331:47)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw.&lt;init&gt;(command-1572003811895331:49)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw.&lt;init&gt;(command-1572003811895331:51)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$read.&lt;init&gt;(command-1572003811895331:53)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$read$.&lt;init&gt;(command-1572003811895331:57)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$read$.&lt;clinit&gt;(command-1572003811895331)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat $line747fd8917310490ea59dce80cb9530a329.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
       "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
       "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1283)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1236)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:889)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:872)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:849)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:660)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:652)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:571)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:606)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:448)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:389)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:247)\n",
       "\tat java.lang.Thread.run(Thread.java:750)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:557)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:673)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:647)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:566)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:557)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:527)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1572003811895331:1)\n\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1572003811895331:43)\n\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1572003811895331:45)\n\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw$$iw.&lt;init&gt;(command-1572003811895331:47)\n\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw$$iw.&lt;init&gt;(command-1572003811895331:49)\n\tat $line747fd8917310490ea59dce80cb9530a329.$read$$iw.&lt;init&gt;(command-1572003811895331:51)\n\tat $line747fd8917310490ea59dce80cb9530a329.$read.&lt;init&gt;(command-1572003811895331:53)\n\tat $line747fd8917310490ea59dce80cb9530a329.$read$.&lt;init&gt;(command-1572003811895331:57)\n\tat $line747fd8917310490ea59dce80cb9530a329.$read$.&lt;clinit&gt;(command-1572003811895331)\n\tat $line747fd8917310490ea59dce80cb9530a329.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $line747fd8917310490ea59dce80cb9530a329.$eval$.$print(&lt;notebook&gt;:6)\n\tat $line747fd8917310490ea59dce80cb9530a329.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1283)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1236)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:889)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:872)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:849)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:660)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:652)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:571)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:606)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:448)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:389)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:247)\n\tat java.lang.Thread.run(Thread.java:750)</div>",
       "errorSummary": "FileNotFoundException: /Filestore/tables",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " %fs\n",
    "\n",
    " ls /Filestore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "271c17fa-9fc1-4f4f-999a-a7d45745fa2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "File uploaded to /FileStore/tables/line_delimited_json.json\n",
    "File uploaded to /FileStore/tables/corrupted_json.json\n",
    "File uploaded to /FileStore/tables/multi_line_correct.json\n",
    "File uploaded to /FileStore/tables/multi_line_incorrect.json\n",
    "File uploaded to /FileStore/tables/single_file_json_extrafields.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53f8565-8d68-4e5d-b5b9-784248af43aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n|age|    name|salary|\n+---+--------+------+\n| 20|  Manish| 20000|\n| 25|  Nikita| 21000|\n| 16|  Pritam| 22000|\n| 35|Prantosh| 25000|\n| 67|  Vikash| 40000|\n+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "    .option('mode','PERMISSIVE')\\\n",
    "    .load('/FileStore/tables/line_delimited_json.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f1c8e4-0459-4327-9fd2-872f2d02a862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+------+\n|age|gender|    name|salary|\n+---+------+--------+------+\n| 20|  null|  Manish| 20000|\n| 25|  null|  Nikita| 21000|\n| 16|  null|  Pritam| 22000|\n| 35|  null|Prantosh| 25000|\n| 67|     M|  Vikash| 40000|\n+---+------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "    .option('mode','PERMISSIVE')\\\n",
    "    .load('/FileStore/tables/single_file_json_extrafields.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4186c676-d003-406f-940f-9eb5fc298d94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n|age|    name|salary|\n+---+--------+------+\n| 20|  Manish| 20000|\n| 25|  Nikita| 21000|\n| 16|  Pritam| 22000|\n| 35|Prantosh| 25000|\n| 67|  Vikash| 40000|\n+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "    .option('mode','PERMISSIVE')\\\n",
    "    .option('multiline','true')\\\n",
    "    .load('/FileStore/tables/multi_line_correct.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "971da1fa-f112-4607-95fa-24e0e7e8650b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"since in incorrect json, the json data is not passed inside list. like - \n",
    " [\n",
    "    {\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        \n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951c532e-0e33-4f87-a02b-41a31339df6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n|age|  name|salary|\n+---+------+------+\n| 20|Manish| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "    .option('mode','PERMISSIVE')\\\n",
    "    .option('multiline','true')\\\n",
    "    .load('/FileStore/tables/multi_line_incorrect.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c0d1da-7d3a-493c-a37b-6b7c2fd64dc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# corrupted json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59da20e1-8331-4f08-b9b7-9b6b9a17c2b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n|age|name  |salary|\n+---+------+------+\n|20 |Manish|20000 |\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "    .option('mode','permissive')\\\n",
    "    .load('/FileStore/tables/corrupted_json.json').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4980d5c7-1751-4e66-a754-f4b8d506e0b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- message: string (nullable = true)\n |-- restaurants: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- restaurant: struct (nullable = true)\n |    |    |    |-- R: struct (nullable = true)\n |    |    |    |    |-- res_id: long (nullable = true)\n |    |    |    |-- apikey: string (nullable = true)\n |    |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |    |-- cuisines: string (nullable = true)\n |    |    |    |-- currency: string (nullable = true)\n |    |    |    |-- deeplink: string (nullable = true)\n |    |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- featured_image: string (nullable = true)\n |    |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |    |-- has_table_booking: long (nullable = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |    |-- location: struct (nullable = true)\n |    |    |    |    |-- address: string (nullable = true)\n |    |    |    |    |-- city: string (nullable = true)\n |    |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |    |-- locality: string (nullable = true)\n |    |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |    |-- zipcode: string (nullable = true)\n |    |    |    |-- menu_url: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- offers: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- photos_url: string (nullable = true)\n |    |    |    |-- price_range: long (nullable = true)\n |    |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |    |-- thumb: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |    |-- votes: string (nullable = true)\n |-- results_found: long (nullable = true)\n |-- results_shown: long (nullable = true)\n |-- results_start: string (nullable = true)\n |-- status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.read.format('json')\\\n",
    "    .option('inferschema','true')\\\n",
    "    .option('mode','permissive')\\\n",
    "    .load('/FileStore/tables/file5.json').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa2f3302-e1f7-4774-9388-922aa79d997c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+-------+------+\n| id|    name|age|salary|address|gender|\n+---+--------+---+------+-------+------+\n|  1|  Manish| 26| 75000|  INDIA|     m|\n|  2|  Nikita| 23|100000|    USA|     f|\n|  3|  Pritam| 22|150000|  INDIA|     m|\n|  4|Prantosh| 17|200000|  JAPAN|     m|\n|  5|  Vikash| 31|300000|    USA|     m|\n|  6|   Rahul| 55|300000|  INDIA|     m|\n|  7|    Raju| 67|540000|    USA|     m|\n|  8| Praveen| 28| 70000|  JAPAN|     m|\n|  9|     Dev| 32|150000|  JAPAN|     m|\n| 10|  Sherin| 16| 25000| RUSSIA|     f|\n| 11|    Ragu| 12| 35000|  INDIA|     f|\n| 12|   Sweta| 43|200000|  INDIA|     f|\n| 13| Raushan| 48|650000|    USA|     m|\n| 14|  Mukesh| 36| 95000| RUSSIA|     m|\n| 15| Prakash| 52|750000|  INDIA|  null|\n+---+--------+---+------+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "        .option('header','true')\\\n",
    "        .option('inferschema','true')\\\n",
    "        .option('mode','PERMISSIBLE')\\\n",
    "        .load('/FileStore/tables/lec8.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b92e1583-64f7-4a0e-b565-d7ad5987bacd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Write Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6a1235-dc7c-43f9-8d0a-60256236a0e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "    .option('header','true')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('path','/FileStore/tables/csv_Write/')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3052bc96-ec47-449e-a491-da46b7d5517e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:557)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:673)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:566)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:557)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:527)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2164186508711884:1)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2164186508711884:43)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2164186508711884:45)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw$$iw.&lt;init&gt;(command-2164186508711884:47)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw.&lt;init&gt;(command-2164186508711884:49)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw.&lt;init&gt;(command-2164186508711884:51)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read.&lt;init&gt;(command-2164186508711884:53)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$.&lt;init&gt;(command-2164186508711884:57)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$.&lt;clinit&gt;(command-2164186508711884)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat $line0900ed50127645f3ad9fa34c0011ca9329.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
       "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
       "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1283)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1236)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:889)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:872)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:849)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:660)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:652)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:571)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:606)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:448)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:389)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:247)\n",
       "\tat java.lang.Thread.run(Thread.java:750)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:557)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:673)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:647)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:566)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:557)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:527)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2164186508711884:1)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2164186508711884:43)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2164186508711884:45)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw$$iw.&lt;init&gt;(command-2164186508711884:47)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw$$iw.&lt;init&gt;(command-2164186508711884:49)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$$iw.&lt;init&gt;(command-2164186508711884:51)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read.&lt;init&gt;(command-2164186508711884:53)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$.&lt;init&gt;(command-2164186508711884:57)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$read$.&lt;clinit&gt;(command-2164186508711884)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$eval$.$print(&lt;notebook&gt;:6)\n\tat $line0900ed50127645f3ad9fa34c0011ca9329.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1283)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1236)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:889)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:872)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:849)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:660)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:652)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:571)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:606)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:448)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:389)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:247)\n\tat java.lang.Thread.run(Thread.java:750)</div>",
       "errorSummary": "FileNotFoundException: /Filestore/tables/csv_Write",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " %fs\n",
    "\n",
    " ls /Filestore/tables/csv_Write/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f74d75d-2a9c-465f-9375-23045fa545f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: [FileInfo(path='dbfs:/FileStore/tables/csv_Write/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1704644588000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/_committed_896402781506046318', name='_committed_896402781506046318', size=110, modificationTime=1704644588000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/_started_896402781506046318', name='_started_896402781506046318', size=0, modificationTime=1704644587000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/part-00000-tid-896402781506046318-07f6768b-e39f-46f1-9b08-742bc07f5ef3-3-1-c000.csv', name='part-00000-tid-896402781506046318-07f6768b-e39f-46f1-9b08-742bc07f5ef3-3-1-c000.csv', size=490, modificationTime=1704644587000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/csv_Write/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227ef4d2-8805-4718-949f-9cad4ef0c26e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.repartition(3).write.format('csv')\\\n",
    "    .option('header','true')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('path','/FileStore/tables/csv_Write/')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d796fc-81df-4ee6-87f9-2c7ba9267b2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: [FileInfo(path='dbfs:/FileStore/tables/csv_Write/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1704645216000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/_committed_7796359927709295272', name='_committed_7796359927709295272', size=381, modificationTime=1704645216000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/_committed_896402781506046318', name='_committed_896402781506046318', size=110, modificationTime=1704644588000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/_started_7796359927709295272', name='_started_7796359927709295272', size=0, modificationTime=1704645215000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/_started_896402781506046318', name='_started_896402781506046318', size=0, modificationTime=1704644587000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/part-00000-tid-7796359927709295272-7c091217-16f4-42e7-a266-7916b622473f-6-1-c000.csv', name='part-00000-tid-7796359927709295272-7c091217-16f4-42e7-a266-7916b622473f-6-1-c000.csv', size=184, modificationTime=1704645215000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/part-00001-tid-7796359927709295272-7c091217-16f4-42e7-a266-7916b622473f-7-1-c000.csv', name='part-00001-tid-7796359927709295272-7c091217-16f4-42e7-a266-7916b622473f-7-1-c000.csv', size=184, modificationTime=1704645215000),\n FileInfo(path='dbfs:/FileStore/tables/csv_Write/part-00002-tid-7796359927709295272-7c091217-16f4-42e7-a266-7916b622473f-8-1-c000.csv', name='part-00002-tid-7796359927709295272-7c091217-16f4-42e7-a266-7916b622473f-8-1-c000.csv', size=190, modificationTime=1704645215000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/csv_Write')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf0c57a-a4d7-4cd6-aa83-b3906ae7cfbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Lec-9: Partitioning and bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fd8d9f6-0f4a-4994-ac87-fbcd589bd10a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### partition on address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee33e837-4077-40cd-82ca-eec411fbe918",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "    .option('header','true')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('path','/FileStore/tables/partition_by_address')\\\n",
    "    .partitionBy('address')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42bcbd6-6aef-4769-9361-af9e118f46e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: False"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm('/FileStore/tables/lec8.csv',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24359088-d8dd-474b-a9ea-50fd386e1205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: [FileInfo(path='dbfs:/FileStore/tables/partition_by_address/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1704647615000),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address/address=INDIA/', name='address=INDIA/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address/address=JAPAN/', name='address=JAPAN/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address/address=RUSSIA/', name='address=RUSSIA/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address/address=USA/', name='address=USA/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/partition_by_address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17dff0a8-7696-4769-b736-caf9ab6c6239",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### partition on address and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "690d5a92-c79e-49db-a3b7-b1816915d218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "    .option('header','true')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('path','/FileStore/tables/partition_by_address_gender')\\\n",
    "    .partitionBy('address','gender')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ecd510b-9407-4c1f-b3dd-24050d5262b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: [FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1704649134000),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/_committed_1639882042462269206', name='_committed_1639882042462269206', size=112, modificationTime=1704648775000),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/_committed_7082773161321007377', name='_committed_7082773161321007377', size=123, modificationTime=1704649134000),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/_started_1639882042462269206', name='_started_1639882042462269206', size=0, modificationTime=1704648774000),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/address=INDIA/', name='address=INDIA/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/address=JAPAN/', name='address=JAPAN/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/address=RUSSIA/', name='address=RUSSIA/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/address=USA/', name='address=USA/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/partition_by_address_gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58dd742-d55b-4638-a6fe-adae12d71873",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: [FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/address=INDIA/gender=__HIVE_DEFAULT_PARTITION__/', name='gender=__HIVE_DEFAULT_PARTITION__/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/address=INDIA/gender=f/', name='gender=f/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_by_address_gender/address=INDIA/gender=m/', name='gender=m/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('dbfs:/FileStore/tables/partition_by_address_gender/address=INDIA/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0213a14-bfd1-4d21-b036-e3b75f195346",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "    .option('header','true')\\\n",
    "    .mode('overwrite')\\\n",
    "    .bucketBy(3,'id')\\\n",
    "    .option('path','/FileStore/tables/bucket_by_id')\\\n",
    "    .saveAsTable('bucket_by_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b861d5-3ed2-4bf5-a0d6-1e0a23e6e2ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: [FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1704652012000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/_committed_3085655768648660591', name='_committed_3085655768648660591', size=306, modificationTime=1704652012000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/_started_3085655768648660591', name='_started_3085655768648660591', size=0, modificationTime=1704652011000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/part-00000-tid-3085655768648660591-06a53c0c-b26a-4aaa-b4e1-8b881adaac79-16-1_00000.c000.csv', name='part-00000-tid-3085655768648660591-06a53c0c-b26a-4aaa-b4e1-8b881adaac79-16-1_00000.c000.csv', size=270, modificationTime=1704652011000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/part-00000-tid-3085655768648660591-06a53c0c-b26a-4aaa-b4e1-8b881adaac79-16-2_00001.c000.csv', name='part-00000-tid-3085655768648660591-06a53c0c-b26a-4aaa-b4e1-8b881adaac79-16-2_00001.c000.csv', size=113, modificationTime=1704652011000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_by_id/part-00000-tid-3085655768648660591-06a53c0c-b26a-4aaa-b4e1-8b881adaac79-16-3_00002.c000.csv', name='part-00000-tid-3085655768648660591-06a53c0c-b26a-4aaa-b4e1-8b881adaac79-16-3_00002.c000.csv', size=114, modificationTime=1704652011000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables/bucket_by_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39f54df3-4175-4f62-817f-2f70914e79d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2164186508711884,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "[Spark Practical]: Read and Write",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
